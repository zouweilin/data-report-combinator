{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in order to use the combinator, make sure to select the correct data report file path. Although the data report exports a CSV, we have used excel formatting to more clearly highlight \n",
    "# what we are going\n",
    "# the other csv files should be in the same folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_report_file_path = \"data report.xlsx\"\n",
    "source_file_paths = glob.glob('*.csv')\n",
    "data_report_df = pd.read_excel(data_report_file_path, dtype=str, na_filter=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SourceFile:\n",
    "    def __init__(self, source_filename=\"\", source_columns = None):\n",
    "        if source_columns is None:\n",
    "            source_columns = []\n",
    "            \n",
    "        self.source_filename = source_filename\n",
    "        self.source_columns =  source_columns\n",
    "\n",
    "class SourceColumn:\n",
    "    def __init__(self, column_name=\"\",merge_type=\"\", merge_destination='',  append_columns = None, series = None):\n",
    "        if append_columns is None:\n",
    "            append_columns = {}\n",
    "            \n",
    "        self.column_name = column_name\n",
    "        self.merge_type = merge_type\n",
    "        self.merge_destination = merge_destination\n",
    "        self.append_columns = append_columns\n",
    "        self.series = series\n",
    "        \n",
    "class NewFile:\n",
    "    def __init__(self, new_filename=\"\", df = pd.DataFrame()):\n",
    "\n",
    "        self.new_filename = new_filename\n",
    "        self.df =  df\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_files = data_report_df['File'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_files = []\n",
    "# creates objects required\n",
    "for link in required_files:\n",
    "    data_report_df_subset = data_report_df[data_report_df['File'] == link]\n",
    "    df = pd.read_csv(link, na_filter=False)\n",
    "    source_file = SourceFile(source_filename=link)\n",
    "    source_files.append(source_file)\n",
    "    for i,row in data_report_df_subset.iterrows():\n",
    "        column_name = row['Column']\n",
    "        merge_type = row['Data Mod']\n",
    "        series = df[column_name]\n",
    "        source_column = SourceColumn(column_name = column_name, merge_type = merge_type, series = series)\n",
    "        source_file.source_columns.append(source_column)\n",
    "        \n",
    "        if row['Data Table'] and row['Data Field']:\n",
    "            source_column.append_columns[row['Data Table']] = row['Data Field']\n",
    "            if merge_type:\n",
    "                source_column.merge_destination = row['Data Field']\n",
    "            \n",
    "        for j, cell in row.iteritems():\n",
    "            if j.startswith('Mod') and cell:\n",
    "                mod_number = j.split(' ')[1]\n",
    "                mod_table_column = f'Table {mod_number}'\n",
    "                mod_field_column = f'Field {mod_number}'\n",
    "                if cell == 'Append':\n",
    "                    source_column.append_columns[row[mod_table_column]]=row[mod_field_column]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_source_column(base_source_column, additional_source_column):\n",
    "    if ',' in base_source_column.merge_type:\n",
    "        merge_delimiter = ','\n",
    "    elif ';' in base_source_column.merge_type:\n",
    "        merge_delimiter = ';'\n",
    "    else:\n",
    "        # is merge delimiter is None, do priority merge\n",
    "        merge_delimiter = None\n",
    "\n",
    "    if merge_delimiter:\n",
    "        base_source_column.series = base_source_column.series + merge_delimiter + additional_source_column.series\n",
    "    else:\n",
    "        base_source_column.series = base_source_column.series.where(base_source_column.series != '', additional_source_column.series)\n",
    "    additional_source_column.append_columns = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for source_file in source_files:\n",
    "    columns_to_merge = {}\n",
    "    # create columns to merge\n",
    "    for source_column in source_file.source_columns:\n",
    "        merge_destination = source_column.merge_destination\n",
    "        if merge_destination == '':\n",
    "            continue\n",
    "        if merge_destination in columns_to_merge:\n",
    "            columns_to_merge[merge_destination].append(source_column)\n",
    "        else:\n",
    "            columns_to_merge[merge_destination] = [source_column]\n",
    "    # orders the merges then performs them\n",
    "    for destination, source_column_list in columns_to_merge.items():\n",
    "        source_column_list = sorted(source_column_list, key=lambda x: x.merge_type)\n",
    "        for i, source_column in enumerate(source_column_list):\n",
    "            if i > 0:\n",
    "                base_source_column = source_column_list[0]\n",
    "                merge_source_column(base_source_column, source_column)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sets up append\n",
    "new_files = {}\n",
    "for source_file in source_files:\n",
    "    new_files_to_series = {}\n",
    "    for source_column in source_file.source_columns:\n",
    "        for new_file, new_column in source_column.append_columns.items():\n",
    "            if new_file in new_files_to_series:\n",
    "                new_files_to_series[new_file].append(source_column)\n",
    "            else:\n",
    "                new_files_to_series[new_file] = [source_column]\n",
    "    # appends done here\n",
    "    for new_file_name, source_columns in new_files_to_series.items():\n",
    "        if new_file_name not in new_files:\n",
    "            new_files[new_file_name] = NewFile(new_filename=new_file_name)\n",
    "            new_file = new_files[new_file_name]\n",
    "        else:\n",
    "            new_file =new_files[new_file_name]\n",
    "            \n",
    "        dataframe_dict = {}\n",
    "        for source_column in source_columns:\n",
    "            dataframe_dict[source_column.append_columns[new_file_name]] = list(source_column.series)\n",
    "        \n",
    "        df_subset = pd.DataFrame(dataframe_dict)\n",
    "        df_subset.insert(0,'Source File', source_file.source_filename)\n",
    "        new_file.df = pd.concat([new_file.df, df_subset], sort=False, ignore_index=True)\n",
    "        new_file.df.astype(int, errors='ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename, new_file in new_files.items():\n",
    "    df = new_file.df\n",
    "    df.to_csv(f'{filename}.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is unrelated to the functionality of the combinator but we needed this to generate the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_df = new_file.df.groupby('Zip').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_df.to_csv(f'{filename}2.csv')\n",
    "df = pd.read_csv('visits2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.groupby('Zip').sum()\n",
    "df.to_csv(f'{filename}2.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
